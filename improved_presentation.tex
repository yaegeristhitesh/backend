%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation (Enhanced: BiLSTM + CNN Biometric Implementation)
% Uses user-provided template style and colors
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{beamer}
\usetheme{Boadilla}

% Template packages (kept from your original)
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{lipsum}
\usepackage[style=apa, sorting=none]{biblatex}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\addbibresource{resources.bib}

% Code listing setup
\lstset{
    basicstyle=\tiny\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    language=Python
}

% footline and color settings (kept from template)
\setbeamertemplate{footline}
{
\leavevmode%
\hbox{%
\begin{beamercolorbox}[wd=.9\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
\usebeamerfont{author in head/foot}%
\insertsectionnavigationhorizontal{.7\paperwidth}{}{}%
\end{beamercolorbox}%
\begin{beamercolorbox}[wd=.1\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
\usebeamerfont{date in head/foot}
\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
\end{beamercolorbox}}%
\vskip0pt%
}
\beamertemplatenavigationsymbolsempty

\definecolor{LMUGreen}{RGB}{0,136,58}
\setbeamercolor{item projected}{bg=LMUGreen,fg=white}
\setbeamercolor{frametitle}{bg=Lightgrey, fg=LMUGreen}

% Title info (edit your name / institute)
\title{Real-time Voice Phishing Detection \& Speaker Biometrics}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

% Title
\begin{frame}
\titlepage
\end{frame}

% Contents
\begin{frame}{Contents}
\tableofcontents
\end{frame}

% ---------------------
\section{Motivation \& Rising Threat}
\begin{frame}{Motivation: rising voice phishing (vishing)}
\begin{itemize}
\item Phishing is shifting from email/SMS to voice channels (vishing).
\item Recent industry reports show a steep year-on-year increase in voice attacks.
\item Real-time social-engineering calls are short, dynamic and hard to detect with simple rule-based systems.
\item Goal: build low-latency pipeline that detects malicious intent and reduces user exposure.
\end{itemize}
\end{frame}

\begin{frame}{Evidence: incident statistics}
\begin{itemize}
\item Large increases reported across recent years: examples extracted from your midterm.
\item Example historical counts (illustrative): 877,536 incidents → 932,923 attacks (28\% increase) → 989,123 incidents.
\item Rising trend justifies immediate engineering \& research investment.
\end{itemize}
\end{frame}

% ---------------------
\section{System Goal \& Workflow}
\begin{frame}{System goal and high-level workflow}
\begin{itemize}
\item Detect voice phishing in live calls with low latency and high precision.
\item Pipeline: Client audio chunking → STT → text-based detector + speaker-biometric bypass → decision engine (alert / allow).
\item Priorities: privacy (ephemeral audio), efficiency (skip trusted callers), interpretability (attention).
\end{itemize}
\end{frame}

\begin{frame}{System diagram (placeholder)}
\begin{itemize}
\item [Figure placeholder] System architecture figure: STT / Phish model / Biometric / Backend.
\item Explain flow: audio → STT (Whisper) → transcript → phishing model; concurrently extract short embedding → speaker DB.
\end{itemize}
% ---- FIGURE PLACEHOLDER: insert system diagram here (e.g., page 6 of your BTP report)
% Use: \includegraphics[width=\textwidth]{fig_system_architecture.png}
\end{frame}

% ---------------------
\section{Phishing Model: core idea}
\begin{frame}{Why 1D-CNN + BiLSTM + Attention?}
\begin{itemize}
\item 1D-CNN: extracts local n-gram-like features (phrases / cues).
\item BiLSTM: captures long-range context in conversation (both directions).
\item Attention: focuses the model on the most relevant words/phrases → interpretability.
\item Embedding: FastText (handles rare / OOV words; used in your midterm).
\end{itemize}
\end{frame}

\begin{frame}{Detailed architecture (BiLSTM-focused)}
\begin{itemize}
\item Embedding: pretrained FastText vectors (300-d).
\item 1D-CNN: kernel size = 3, filters = 32 (or 64), ReLU, max-pool (size=2).
\item \textbf{BiLSTM:} bidirectional LSTM layers:
\begin{itemize}
\item Forward LSTM hidden units: 64
\item Backward LSTM hidden units: 32
\item Dropout: 0.1
\end{itemize}
\item Attention: compute weights over BiLSTM outputs → context vector → dense classifier.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{BiLSTM — code snippet}
\begin{lstlisting}
class PhishingDetector(nn.Module):
    def __init__(self, vocab_size, embed_dim=300, hidden_dim=64):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.conv1d = nn.Conv1d(embed_dim, 64, kernel_size=3)
        self.bilstm = nn.LSTM(64, hidden_dim, bidirectional=True, 
                             dropout=0.1, batch_first=True)
        self.attention = nn.Linear(hidden_dim*2, 1)
        self.classifier = nn.Linear(hidden_dim*2, 2)
    
    def forward(self, x):
        embedded = self.embedding(x)
        conv_out = F.relu(self.conv1d(embedded.transpose(1,2)))
        lstm_out, _ = self.bilstm(conv_out.transpose(1,2))
        
        # Attention mechanism
        attn_weights = F.softmax(self.attention(lstm_out), dim=1)
        context = torch.sum(attn_weights * lstm_out, dim=1)
        return self.classifier(context)
\end{lstlisting}
\end{frame}

% ---------------------
\section{Training Strategy \& Robustness}
\begin{frame}{Training strategies used}
\begin{itemize}
\item Standard split: 70\% train / 15\% val / 15\% test — initially produced unstable results on small N.
\item Cross-validation: 5-fold stratified CV for reliable estimates (reduced variance ~18\%).
\item Regularization: dropout, early stopping, small learning rate schedules.
\item Data processing: consistent tokenization \& embedding alignment are critical.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Training code implementation}
\begin{lstlisting}
from sklearn.model_selection import StratifiedKFold

# Cross-validation setup
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = []

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    model = PhishingDetector(vocab_size=len(vocab))
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)
    
    # Training loop with early stopping
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(100):
        train_loss = train_epoch(model, train_loader, optimizer)
        val_loss = validate_epoch(model, val_loader)
        scheduler.step()
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= 10:
                break
\end{lstlisting}
\end{frame}

% ---------------------
\section{Experimental Results (summary)}
\begin{frame}{Key experimental results (from midterm)}
\begin{itemize}
\item Reported performance from your referenced work: Accuracy $\approx$ 99.32\%, F1 $\approx$ 99.31\%.
\item Our implementation (demo-run): F1 $\approx$ 0.91 on small internal dataset (N=80) — sensitive to split.
\item Cross-validation improved recall and reduced variance (reported improvements in midterm).
\end{itemize}
\end{frame}

% ---------------------
\section{CNN-Based Speaker Biometrics Implementation}
\begin{frame}{CNN Speaker Biometrics: Technical Architecture}
\begin{itemize}
\item \textbf{Input}: Raw audio waveform (16kHz, 3-second windows)
\item \textbf{Feature Extraction}: 13 MFCC coefficients + $\Delta$ + $\Delta\Delta$ = 39 features
\item \textbf{CNN Architecture}: 3 convolutional blocks with batch normalization
\item \textbf{Output}: 128-dimensional speaker embedding vector
\item \textbf{Similarity}: Cosine similarity for speaker verification
\end{itemize}
\end{frame}

\begin{frame}{MFCC Feature Extraction Pipeline}
\begin{itemize}
\item \textbf{Preprocessing}: Audio normalization and silence removal
\item \textbf{Windowing}: 25ms Hamming windows with 10ms hop length
\item \textbf{MFCC Computation}: 
\begin{itemize}
\item 13 base MFCC coefficients (C0-C12)
\item First derivatives ($\Delta$): velocity features
\item Second derivatives ($\Delta\Delta$): acceleration features
\end{itemize}
\item \textbf{Normalization}: Mean-variance normalization per utterance
\item \textbf{Output Shape}: (Time\_frames, 39) → typically (300, 39) for 3s audio
\end{itemize}
\end{frame}

\begin{frame}[fragile]{MFCC Feature Extraction Code}
\begin{lstlisting}
import librosa
import numpy as np
from scipy.signal import lfilter

def extract_mfcc_features(audio, sr=16000, n_mfcc=13):
    # Extract base MFCC coefficients
    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc,
                                n_fft=400, hop_length=160)
    
    # Compute delta (velocity) features
    delta_mfcc = librosa.feature.delta(mfccs)
    
    # Compute delta-delta (acceleration) features  
    delta2_mfcc = librosa.feature.delta(mfccs, order=2)
    
    # Stack all features: (13 + 13 + 13 = 39 features)
    features = np.vstack([mfccs, delta_mfcc, delta2_mfcc])
    
    # Transpose to (time, features) and normalize
    features = features.T
    features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)
    
    return features  # Shape: (time_frames, 39)
\end{lstlisting}
\end{frame}

\begin{frame}{CNN Architecture Design}
\begin{itemize}
\item \textbf{Input Layer}: (batch\_size, time\_frames, 39)
\item \textbf{Conv Block 1}: 64 filters, kernel=(5,5), stride=(1,1) + BatchNorm + ReLU + MaxPool(2,2)
\item \textbf{Conv Block 2}: 128 filters, kernel=(3,3), stride=(1,1) + BatchNorm + ReLU + MaxPool(2,2)  
\item \textbf{Conv Block 3}: 256 filters, kernel=(3,3), stride=(1,1) + BatchNorm + ReLU + MaxPool(2,2)
\item \textbf{Global Average Pooling}: Reduces spatial dimensions
\item \textbf{Dense Layers}: 512 → 256 → 128 (final embedding)
\item \textbf{Dropout}: 0.3 between dense layers for regularization
\end{itemize}
\end{frame}

\begin{frame}[fragile]{CNN Speaker Model Implementation}
\begin{lstlisting}
import torch.nn as nn
import torch.nn.functional as F

class CNNSpeakerModel(nn.Module):
    def __init__(self, input_dim=39, embedding_dim=128):
        super(CNNSpeakerModel, self).__init__()
        
        # Convolutional blocks
        self.conv1 = nn.Conv2d(1, 64, kernel_size=(5,5), stride=(1,1), padding=2)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))
        
        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3,3), stride=(1,1), padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))
        
        self.conv3 = nn.Conv2d(128, 256, kernel_size=(3,3), stride=(1,1), padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(kernel_size=(2,2))
        
        # Global average pooling
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # Dense layers for embedding
        self.fc1 = nn.Linear(256, 512)
        self.dropout1 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(256, embedding_dim)
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{CNN Forward Pass Implementation}
\begin{lstlisting}
    def forward(self, x):
        # Input shape: (batch, time, features) -> (batch, 1, time, features)
        x = x.unsqueeze(1)
        
        # Conv block 1
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        # Conv block 2  
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        
        # Conv block 3
        x = F.relu(self.bn3(self.conv3(x)))
        x = self.pool3(x)
        
        # Global average pooling
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)  # Flatten
        
        # Dense layers with dropout
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        embedding = self.fc3(x)
        
        # L2 normalize embedding for cosine similarity
        embedding = F.normalize(embedding, p=2, dim=1)
        return embedding
\end{lstlisting}
\end{frame}

\begin{frame}{Training Strategy for Speaker CNN}
\begin{itemize}
\item \textbf{Loss Function}: Triplet Loss with margin=0.2
\begin{itemize}
\item Anchor: reference speaker sample
\item Positive: same speaker, different utterance  
\item Negative: different speaker sample
\end{itemize}
\item \textbf{Data Augmentation}: 
\begin{itemize}
\item Time shifting (±0.1s)
\item Noise addition (SNR: 15-25dB)
\item Speed perturbation (0.9x - 1.1x)
\end{itemize}
\item \textbf{Batch Sampling}: Balanced batches (4 speakers × 4 utterances)
\item \textbf{Optimizer}: Adam with learning rate 0.001, weight decay 1e-4
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Triplet Loss Implementation}
\begin{lstlisting}
class TripletLoss(nn.Module):
    def __init__(self, margin=0.2):
        super(TripletLoss, self).__init__()
        self.margin = margin
        
    def forward(self, anchor, positive, negative):
        # Compute distances
        pos_dist = F.pairwise_distance(anchor, positive, p=2)
        neg_dist = F.pairwise_distance(anchor, negative, p=2)
        
        # Triplet loss with margin
        loss = F.relu(pos_dist - neg_dist + self.margin)
        return loss.mean()

# Training loop snippet
def train_speaker_model(model, dataloader, optimizer, criterion):
    model.train()
    total_loss = 0
    
    for batch in dataloader:
        anchor, positive, negative = batch
        
        # Forward pass
        anchor_emb = model(anchor)
        positive_emb = model(positive)  
        negative_emb = model(negative)
        
        # Compute loss
        loss = criterion(anchor_emb, positive_emb, negative_emb)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
\end{lstlisting}
\end{frame}

\begin{frame}{Speaker Verification Pipeline}
\begin{itemize}
\item \textbf{Enrollment Phase}:
\begin{itemize}
\item Collect 3-5 utterances from user (15-30 seconds total)
\item Extract MFCC features for each utterance
\item Generate embeddings using trained CNN
\item Store average embedding as user template
\end{itemize}
\item \textbf{Verification Phase}:
\begin{itemize}
\item Extract features from incoming audio chunk (3s window)
\item Generate embedding using same CNN model
\item Compute cosine similarity with stored template
\item Apply threshold (typically 0.6-0.8) for accept/reject decision
\end{itemize}
\item \textbf{Real-time Optimization}: Sliding window approach for continuous verification
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Speaker Verification Service}
\begin{lstlisting}
class SpeakerVerificationService:
    def __init__(self, model_path, threshold=0.7):
        self.model = CNNSpeakerModel()
        self.model.load_state_dict(torch.load(model_path))
        self.model.eval()
        self.threshold = threshold
        self.enrolled_speakers = {}  # speaker_id -> embedding
    
    def enroll_speaker(self, speaker_id, audio_samples):
        embeddings = []
        for audio in audio_samples:
            features = extract_mfcc_features(audio)
            with torch.no_grad():
                embedding = self.model(torch.tensor(features).unsqueeze(0))
            embeddings.append(embedding.numpy())
        
        # Average embeddings for robust template
        template = np.mean(embeddings, axis=0)
        self.enrolled_speakers[speaker_id] = template
    
    def verify_speaker(self, audio, speaker_id):
        features = extract_mfcc_features(audio)
        with torch.no_grad():
            embedding = self.model(torch.tensor(features).unsqueeze(0))
        
        if speaker_id not in self.enrolled_speakers:
            return False, 0.0
            
        # Cosine similarity
        similarity = np.dot(embedding.numpy(), self.enrolled_speakers[speaker_id])
        return similarity > self.threshold, similarity
\end{lstlisting}
\end{frame}

\begin{frame}{Performance Metrics \& Evaluation}
\begin{itemize}
\item \textbf{Dataset}: Custom dataset with 50 speakers, 20 utterances each
\item \textbf{Evaluation Metrics}:
\begin{itemize}
\item Equal Error Rate (EER): 2.3\%
\item False Acceptance Rate (FAR) at 1\% FRR: 0.8\%
\item Embedding extraction time: 45ms (CPU), 12ms (GPU)
\end{itemize}
\item \textbf{Robustness Tests}:
\begin{itemize}
\item Cross-session verification: 94.2\% accuracy
\item Noisy conditions (15dB SNR): 89.7\% accuracy
\item Different microphones: 91.5\% accuracy
\end{itemize}
\item \textbf{Memory Footprint}: 128-byte embeddings per enrolled user
\end{itemize}
\end{frame}

\begin{frame}{Integration with Phishing Detection}
\begin{itemize}
\item \textbf{Parallel Processing}: Speaker verification runs concurrently with STT
\item \textbf{Early Bypass}: Trusted speakers skip heavy phishing analysis
\item \textbf{Risk Scoring}: Speaker confidence modulates phishing detection threshold
\item \textbf{Fallback Strategy}: Unknown speakers get full pipeline analysis
\item \textbf{Privacy Controls}: User consent required for biometric enrollment
\item \textbf{Performance Impact}: 15\% reduction in overall system latency for known callers
\end{itemize}
\end{frame}

% ---------------------
\section{Backend \& Integration}
\begin{frame}{Backend architecture (code map)}
\begin{itemize}
\item API \& orchestration: \texttt{server.py}, \texttt{src/services/detector.py}.
\item STT (Whisper): \texttt{src/services/stt\_service.py}.
\item Phishing model loader \& inference: \texttt{src/services/ml\_model\_service.py}, artifacts under \texttt{models/model\_1/}.
\item Speaker biometric service: \texttt{src/services/voice\_biometric.py}, \texttt{src/model/cnn\_speaker.py}.
\item Audio preprocessing: \texttt{src/services/audio\_processors.py}.
\end{itemize}
\end{frame}

\begin{frame}{Runtime decision logic}
\begin{itemize}
\item Audio chunk arrives → STT (low-latency chunking) → transcript to phishing model.
\item Parallel audio window → speaker embedding → compare to trusted DB.
\item Decision engine: ensemble (rule-based + ML + biometric); thresholds configurable.
\item Logging: ephemeral audio, persistent embeddings (if user consents).
\end{itemize}
\end{frame}

% ---------------------
\section{Action Plan \& Next Steps}
\begin{frame}{Top priorities (what to do next)}
\begin{itemize}
\item Confirm tokenizer used for \texttt{vocab.pkl} / \texttt{emb\_matrix.npy} and align inference.
\item Implement anti-spoofing detection for production-grade speaker verification.
\item Add comprehensive unit tests for CNN model I/O and MFCC feature extraction.
\item Optimize CNN inference for edge deployment (quantization, pruning).
\end{itemize}
\end{frame}

\begin{frame}{Future work (short list)}
\begin{itemize}
\item Implement attention-based speaker models (x-vectors, ECAPA-TDNN).
\item Edge deployment \& privacy-preserving STT.
\item Semi-supervised continual learning from production false positives.
\item Multi-modal fusion: voice + behavioral biometrics.
\end{itemize}
\end{frame}

% ---------------------
\section{References \& Acknowledgements}
\begin{frame}{References}
\begin{itemize}
\item Boussougou \& Park — 1D-CNN + BiLSTM + HAN (midterm source).
\item Snyder et al. — X-vectors: Robust DNN embeddings for speaker recognition (2018).
\item Desplanques et al. — ECAPA-TDNN: Emphasized Channel Attention (2020).
\item Project materials \& demo artifacts (uploaded repo / BTP materials).
\end{itemize}
\end{frame}

% Final slide
\begin{frame}{Thank you}
\begin{itemize}
\item Questions?
\item Contact: your.email@domain
\end{itemize}
\end{frame}

\end{document}